The project aimed to create an intelligent image captioning system that generates accurate and grammatically correct descriptions for input images by integrating computer vision and natural language processing through an encoder-decoder architecture. Utilizing a pre-trained InceptionV3 model for feature extraction and an LSTM-based decoder for sentence generation, the system also translates captions into multiple languages and converts them to speech with the Google Translate API and gTTS. The methodology included data preparation, feature extraction, caption tokenization, and training the model using greedy decoding, with performance evaluated via BLEU scores, indicating the model's effectiveness in producing coherent captions for various images. Although the implementation is basic, it serves as a foundation for future enhancements that could further improve caption accuracy and contextual relevance through advanced feature extractors and object detection modules.
